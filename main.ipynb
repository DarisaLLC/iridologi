{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Define Data Path\\n2. Read image in path\\n3. Call process_img function to process image & get dataframe\\n4. Split dataset into train-test set using train_test_split using split value: 0.2\\n5. Apply PCA to train and test set\\n6. Train Backpropagation model using the PCA train set\\n7. Test the model using PCA test set\\n8. Check the accuracy of current model\\n9. Repeat 4-7 10 times & compare accuracy\\n10. Use the highest accuracy train-test set as the main train-test set to optimize.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Define Data Path\n",
    "2. Read image in path\n",
    "3. Call process_img function to process image & get dataframe\n",
    "4. Split dataset into train-test set using train_test_split using split value: 0.2\n",
    "5. Apply PCA to train and test set\n",
    "6. Train Backpropagation model using the PCA train set\n",
    "7. Test the model using PCA test set\n",
    "8. Check the accuracy of current model\n",
    "9. Repeat 4-7 10 times & compare accuracy\n",
    "10. Use the highest accuracy train-test set as the main train-test set to optimize.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display Image\n",
    "def display_img(image):\n",
    "    plt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "# Read File in Path\n",
    "def read_file(path):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        img = cv.imread(os.path.join(path, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "# Read CSV File\n",
    "def readCSV(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert Feature DataFrame CSV into float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert DataFrame label into int        \n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "def rotate(image, angle):\n",
    "    height, width = image.shape[:2]\n",
    "    rot_mat = cv.getRotationMatrix2D((width/2, height/2), angle, 1)\n",
    "    rotated_img = cv.warpAffine(image, rot_mat, (width,height))\n",
    "    return rotated_img\n",
    "\n",
    "# Apply Canny with Automatic Parameter\n",
    "def auto_canny(image, sigma=0.33):\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    "\n",
    "    # apply automatic Canny edge detection using computed median\n",
    "    lower = int(max(0, (1.0-sigma) * v))\n",
    "    upper = int(min(255, (1.0+sigma) * v))\n",
    "    canny = cv.Canny(image, lower, upper)\n",
    "\n",
    "    return canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nProcess Image Steps:\\n1. Grayscaling\\n2. CLAHE Histogram Equalizing\\n3. MedianBlur\\n4. Mask Pupil\\n5. Convert Iris Image to Cartesian (center, radius = image.shape[0]/2)\\n6. Crop 2~3 o'clock region\\n7. Crop pupil area\\n8. Crop ROI\\n9. Apply autocanny\\n10. Flatten image as feature\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Process Image Steps:\n",
    "1. Grayscaling\n",
    "2. CLAHE Histogram Equalizing\n",
    "3. MedianBlur\n",
    "4. Mask Pupil\n",
    "5. Convert Iris Image to Cartesian (center, radius = image.shape[0]/2)\n",
    "6. Crop 2~3 o'clock region\n",
    "7. Crop pupil area\n",
    "8. Crop ROI\n",
    "9. Apply autocanny\n",
    "10. Flatten image as feature\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image, name):\n",
    "    # Grayscaling\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    cv.imwrite('./Result/gray/'+name, gray)\n",
    "\n",
    "    # Equalize Histogram\n",
    "    clahe = cv.createCLAHE(clipLimit=5.0, tileGridSize=(5,5))\n",
    "    hist = clahe.apply(gray)\n",
    "    cv.imwrite('./Result/hist/'+name, hist)\n",
    "\n",
    "    # Blur Image (Reduce Noise)\n",
    "    blur = cv.medianBlur(hist, 5)\n",
    "    cv.imwrite('./Result/blur/'+name, blur)\n",
    "\n",
    "    # Mask Pupil\n",
    "    _, thresh = cv.threshold(blur, 10, 255, cv.THRESH_BINARY_INV)\n",
    "    contours, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    maxContour = 0\n",
    "    for contour in contours:\n",
    "        contourSize = cv.contourArea(contour)\n",
    "        if contourSize > maxContour:\n",
    "            maxContour = contourSize\n",
    "            maxContourData = contour\n",
    "    ## find enclosing circle of pupil contour\n",
    "    (x,y) ,r = cv.minEnclosingCircle(maxContourData)\n",
    "    center = (int(x), int(y))\n",
    "    radius = int(r)\n",
    "    \n",
    "    img = blur.copy()\n",
    "    masked_pupil = cv.circle(img, center, radius+10, (255,255,255), -1)\n",
    "    cv.imwrite('./Result/masked_pupil/'+name, masked_pupil)\n",
    "\n",
    "    return masked_pupil\n",
    "\n",
    "def segmentation(image, name):\n",
    "    center = (int(image.shape[0]/2),int(image.shape[0]/2))\n",
    "    radius = int(image.shape[0]/2)\n",
    "    \n",
    "    # Convert to cartesian\n",
    "    cartesian = cv.linearPolar(image, center, radius, cv.WARP_FILL_OUTLIERS)\n",
    "    cartesian = rotate(cartesian, -90)\n",
    "    cv.imwrite('./Result/cartesian/'+name, cartesian)\n",
    "    \n",
    "    # Crop Target\n",
    "    [y, x] = cartesian.shape\n",
    "    target = cartesian[0:int(y), 1:int(x/12)]\n",
    "    cv.imwrite('./Result/target/'+name, target)\n",
    "    \n",
    "    # Crop Pupil Area\n",
    "    _, thresh = cv.threshold(target, 250, 255, cv.THRESH_BINARY)\n",
    "    contours, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    maxContour = 0\n",
    "    for contour in contours:\n",
    "        contourSize = cv.contourArea(contour)\n",
    "        if contourSize > maxContour:\n",
    "            maxContour = contourSize\n",
    "            maxContourData = contour\n",
    "    rect = cv.boundingRect(maxContourData)\n",
    "    x,y,w,h = rect\n",
    "    crop_pupil = target[y+h:, 0:]\n",
    "    cv.imwrite('./Result/crop_pupil/'+name, crop_pupil)\n",
    "    \n",
    "    # Specify ROI\n",
    "    [y,x] = crop_pupil.shape\n",
    "    roi = crop_pupil[0:int(y/2), 0:]\n",
    "    cv.imwrite('./Result/roi/'+name, roi)\n",
    "    \n",
    "    #Resize ROI\n",
    "    roi_res = cv.resize(roi, (50,50))\n",
    "    cv.imwrite('./Result/roi_res/'+name, roi_res)\n",
    "    \n",
    "    return roi_res\n",
    "\n",
    "def find_feat(image, name):\n",
    "    canny = auto_canny(image)\n",
    "    cv.imwrite('./Result/canny/'+name, canny)\n",
    "    # Flatten Image Array\n",
    "    feature = canny.flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS IMAGE HANDLER\n",
    "def process_image(path, label):\n",
    "    # Read File on Path\n",
    "    data = read_file(path)\n",
    "    print('Folder {} Contains {} Images'.format(label, len(data)))\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for n, file in enumerate(data):\n",
    "        name = label+'_{}.JPG'.format(n)\n",
    "        # Preprocessing Image in Data\n",
    "        masked_pupil = preprocessing(file, name)\n",
    "        # Segmenting Image ROI\n",
    "        roi = segmentation(masked_pupil, name)\n",
    "        # Find Image Feature\n",
    "        feature = find_feat(roi, name)\n",
    "        features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def process_df(ada_features, tidak_features):\n",
    "    # Build DataFrame From Feature Arrays\n",
    "    print('Building DataFrame')\n",
    "    df_tidak = pd.DataFrame(np.array(tidak_features))\n",
    "    df_tidak['label'] = 0\n",
    "    df_ada = pd.DataFrame(np.array(ada_features))\n",
    "    df_ada['label'] = 1\n",
    "    # Concatenate DataFrame\n",
    "    df_feat = pd.concat([df_tidak, df_ada], ignore_index=True)\n",
    "    df_feat.to_csv('./features_df.csv', header=False, index=False)\n",
    "    print('DataFrame Built\\n')\n",
    "\n",
    "    # Normalize DataFrame\n",
    "    print('Normalizing DataFrame')\n",
    "    X = df_feat.iloc[:, :-1]\n",
    "    y = df_feat.iloc[:, -1]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    dataset = pd.concat([X,y], axis=1)\n",
    "    dataset.to_csv('./normalized_df.csv', header=False, index=False)\n",
    "    print('DataFrame  Normalized!\\n')\n",
    "\n",
    "def train_test(path):\n",
    "    dataset = readCSV(path)\n",
    "    for i in range(len(dataset[0])-1):\n",
    "        str_column_to_float(dataset,i)\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "#     Split Features From Label\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    X = np.array(dataset.iloc[:, :-1])\n",
    "    y = np.array(dataset.iloc[:, -1])\n",
    "\n",
    "#     Define the StratifiedKFold train-test splitter and split Dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train).reset_index().drop('index', axis=1)\n",
    "    X_test = pd.DataFrame(X_test).reset_index().drop('index', axis=1)\n",
    "    y_train = pd.DataFrame(y_train).reset_index().drop('index', axis=1)\n",
    "    y_test = pd.DataFrame(y_test).reset_index().drop('index', axis=1)\n",
    "\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    train_df.to_csv('./train_test_set/train_df.csv', header=False, index=False)\n",
    "    test_df = pd.concat([X_test, y_test], axis=1)\n",
    "    test_df.to_csv('./train_test_set/test_df.csv', header=False, index=False)\n",
    "    \n",
    "    print('Dataset Splitted Into Train-Test Set!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(X_train, X_test, target_size):\n",
    "    print('PCA Target Size = {}'.format(target_size))\n",
    "    pca = PCA(target_size)\n",
    "\n",
    "    print('Transforming Train Dataset')\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print('Transforming Test Dataset')\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import seed\n",
    "from math import exp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize Network\n",
    "def initialize_network(n_inputs, n_hidden, n_layers, n_outputs):\n",
    "    network = list()\n",
    "    seed(0)\n",
    "#     for i in range (n_layers):\n",
    "#         hidden_layer = [{'weights': [round(random.uniform(0,0.5),2) for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "#         if i > 0:\n",
    "#             hidden_layer = [{'weights': [round(random.uniform(0,0.5),2) for i in range(n_hidden+1)]} for i in range(n_hidden)]\n",
    "#         network.append(hidden_layer)\n",
    "#     output_layer = [{'weights': [round(random.uniform(0,0.5),2) for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
    "    for i in range (n_layers):\n",
    "        hidden_layer = [{'weights': [round(random.random(),2) for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "        if i > 0:\n",
    "            hidden_layer = [{'weights': [round(random.random(),2) for i in range(n_hidden+1)]} for i in range(n_hidden)]\n",
    "        network.append(hidden_layer)\n",
    "    output_layer = [{'weights': [round(random.random(),2) for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# FORWARD PROPAGATE\n",
    "# 1. Neuron Activation\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# 2. Neuron Transfer\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0+exp(-activation))\n",
    "\n",
    "# 3. Forward Propagation\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# BACK PROPAGATE\n",
    "# 1. Transfer Derivative\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# 2. Error Backpropagation\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i !=len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "            \n",
    "# 3. Update Weights\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# TRAIN NETWORK\n",
    "def train_network(network, train, l_rate, loss_limit, n_outputs):\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        epoch+=1\n",
    "        if epoch == 20000:\n",
    "            break\n",
    "        if sum_error <= loss_limit:\n",
    "            break\n",
    "    print('>epoch=%d, l_rate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "        \n",
    "# MAKING PREDICTION\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# CHECK ACCURACY\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct/float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKPROPAGATION HANDLER\n",
    "def back_propagation(train, test, l_rate, loss_limit, n_layers, n_hidden, name):\n",
    "    n_inputs = len(train[0])-1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_layers, n_outputs)\n",
    "    train_network(network, train, l_rate, loss_limit, n_outputs)\n",
    "    pd.DataFrame(np.array(network)).to_csv(name, header=False, index=False)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    print('PREDICTIONS:')\n",
    "    print(predictions)\n",
    "    expected = [row[-1] for row in test]\n",
    "    print('EXPECTED:')\n",
    "    print(expected)\n",
    "    accuracy = accuracy_metric(expected, predictions)\n",
    "    print('Accuracy = %.3f%%' %(accuracy))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(confusion_matrix(expected, predictions))\n",
    "    tn, fp, fn, tp = confusion_matrix(expected, predictions).ravel()\n",
    "    print('TN={}, FP={}, FN={}, TP={}'.format(tn, fp, fn, tp))\n",
    "    return network, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMain Function\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Main Function\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data Ada\n",
      "Folder ada Contains 55 Images\n",
      "Done!\n",
      "Processing Data Tidak\n",
      "Folder tidak Contains 55 Images\n",
      "Done!\n",
      "\n",
      "Building DataFrame\n",
      "DataFrame Built\n",
      "\n",
      "Normalizing DataFrame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chali\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame  Normalized!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Data Path\n",
    "data_ada = './Data_ext/ada/'\n",
    "data_tidak = './Data_ext/tidak/'\n",
    "\n",
    "# -----PROCESSING IMAGE-----\n",
    "print('Processing Data Ada')\n",
    "ada = 'ada'\n",
    "ada_features = process_image(data_ada, ada)\n",
    "print('Done!')\n",
    "print('Processing Data Tidak')\n",
    "tidak = 'tidak'\n",
    "tidak_features = process_image(data_tidak, tidak)\n",
    "print('Done!\\n')\n",
    "\n",
    "# -----BUILD DATAFRAME-----\n",
    "process_df(ada_features, tidak_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Target Size = 10\n",
      "Transforming Train Dataset\n",
      "Transforming Test Dataset\n",
      "DataFrame Decomposed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Variable\n",
    "target_pca = 10\n",
    "\n",
    "# path = './normalized_df.csv'\n",
    "# Split Dataset into Train-Test Set\n",
    "# train_test(path)\n",
    "\n",
    "# ------APPLY PCA-------\n",
    "train_path = './train_test_fix/train_df.csv'\n",
    "test_path = './train_test_fix/test_df.csv'\n",
    "\n",
    "# Read Train Set\n",
    "train_df = readCSV(train_path)\n",
    "for i in range(len(train_df[0])-1):\n",
    "    str_column_to_float(train_df, i)\n",
    "str_column_to_int(train_df, len(train_df[0])-1)\n",
    "# Read Test Set\n",
    "test_df = readCSV(test_path)\n",
    "for i in range(len(test_df[0])-1):\n",
    "    str_column_to_float(test_df, i)\n",
    "str_column_to_int(test_df, len(test_df[0])-1)\n",
    "\n",
    "# Split Features From Label\n",
    "train_df = pd.DataFrame(train_df)\n",
    "test_df = pd.DataFrame(test_df)\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test = apply_pca(X_train, X_test, target_pca)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# Reconstruct DataFrame\n",
    "pca_train_set = pd.concat([X_train,y_train], axis=1, ignore_index=True)\n",
    "train_name = './train_test_fix/PCA_TRAIN_MODEL.csv'\n",
    "pca_train_set.to_csv(train_name, header=False, index=False)\n",
    "\n",
    "pca_test_set = pd.concat([X_test, y_test], axis=1, ignore_index=True)\n",
    "test_name = './train_test_fix/PCA_TEST_MODEL.csv'\n",
    "pca_test_set.to_csv(test_name, header=False, index=False)\n",
    "\n",
    "print('DataFrame Decomposed!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(target_pca, l_rate, loss_limit, n_layers, n_hiddens):\n",
    "    # -----START BACKPROPAGATION-----\n",
    "    start = time.time()\n",
    "\n",
    "    train_name = './train_test_fix/pca{}/PCA_TRAIN_MODEL.csv'.format(target_pca)\n",
    "    test_name = './train_test_fix/pca{}/PCA_TEST_MODEL.csv'.format(target_pca)\n",
    "\n",
    "    pca_train_set = readCSV(train_name)\n",
    "    for i in range(len(pca_train_set[0])-1):\n",
    "        str_column_to_float(pca_train_set, i)\n",
    "    str_column_to_int(pca_train_set, len(pca_train_set[0])-1)\n",
    "\n",
    "    pca_test_set = readCSV(test_name)\n",
    "    for i in range(len(pca_test_set[0])-1):\n",
    "        str_column_to_float(pca_test_set, i)\n",
    "    str_column_to_int(pca_test_set, len(pca_test_set[0])-1)\n",
    "\n",
    "    # print('\\nl_rate = {}, n_epoch = {}, n_hidden = {}\\n'.format(l_rate, n_epoch, n_hidden))\n",
    "    print('\\npca = {}, l_rate = {}, loss_limit = {}, n_layers = {}, n_hiddens = {}\\n'.format(target_pca, l_rate, loss_limit, n_layers, n_hiddens))\n",
    "\n",
    "    # network = back_propagation_tts(pca_train_set, pca_test_set, l_rate, n_epoch, n_layers, n_hidden)\n",
    "    name = './train_test_fix/network{}-{}.csv'.format(target_pca, n_hiddens)\n",
    "    network, accuracy = back_propagation(pca_train_set, pca_test_set, l_rate, loss_limit, n_layers, n_hiddens, name)\n",
    "    stop = time.time()\n",
    "\n",
    "    print('Elapsed Time: {}s'.format(stop-start))\n",
    "    \n",
    "def runloop(target_pca, l_rate, loss_limit, n_layers, n_hiddens):\n",
    "    for n, i in enumerate(range(5)):\n",
    "        # -----START BACKPROPAGATION-----\n",
    "        start = time.time()\n",
    "\n",
    "        train_name = './train_test_fix/pca{}/PCA_TRAIN_MODEL.csv'.format(target_pca)\n",
    "        test_name = './train_test_fix/pca{}/PCA_TEST_MODEL.csv'.format(target_pca)\n",
    "\n",
    "        pca_train_set = readCSV(train_name)\n",
    "        for i in range(len(pca_train_set[0])-1):\n",
    "            str_column_to_float(pca_train_set, i)\n",
    "        str_column_to_int(pca_train_set, len(pca_train_set[0])-1)\n",
    "\n",
    "        pca_test_set = readCSV(test_name)\n",
    "        for i in range(len(pca_test_set[0])-1):\n",
    "            str_column_to_float(pca_test_set, i)\n",
    "        str_column_to_int(pca_test_set, len(pca_test_set[0])-1)\n",
    "\n",
    "        # print('\\nl_rate = {}, n_epoch = {}, n_hidden = {}\\n'.format(l_rate, n_epoch, n_hidden))\n",
    "        print('#{}'.format(n+1))\n",
    "        print('\\npca = {}, l_rate = {}, loss_limit = {}, n_layers = {}, n_hiddens = {}\\n'.format(target_pca, l_rate, loss_limit, n_layers, n_hiddens))\n",
    "\n",
    "        # network = back_propagation_tts(pca_train_set, pca_test_set, l_rate, n_epoch, n_layers, n_hidden)\n",
    "        name = './train_test_fix/network{}-{}_{}.csv'.format(target_pca, n_hiddens, n+1)\n",
    "        network, accuracy = back_propagation(pca_train_set, pca_test_set, l_rate, loss_limit, n_layers, n_hiddens, name)\n",
    "        stop = time.time()\n",
    "\n",
    "        print('Elapsed Time: {}s'.format(stop-start))\n",
    "        print('\\n======================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_pca = 20\n",
    "# l_rate = 0.01\n",
    "# loss_limit = 0.1\n",
    "# n_layers = 1\n",
    "\n",
    "# n_hiddens = 20\n",
    "# runloop(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "\n",
    "# n_hiddens = 30\n",
    "# runloop(target_pca, l_rate, loss_limit, n_layers, n_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_pca = 30\n",
    "# l_rate = 0.01\n",
    "# loss_limit = 0.1\n",
    "# n_layers = 1\n",
    "\n",
    "# n_hiddens = 20\n",
    "# runloop(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "\n",
    "# n_hiddens = 30\n",
    "# runloop(target_pca, l_rate, loss_limit, n_layers, n_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_pca = 40\n",
    "# l_rate = 0.01\n",
    "# loss_limit = 0.1\n",
    "# n_layers = 1\n",
    "\n",
    "# n_hiddens = 20\n",
    "# runloop(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "\n",
    "# n_hiddens = 30\n",
    "# runloop(target_pca, l_rate, loss_limit, n_layers, n_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pca = 10, l_rate = 0.01, loss_limit = 0.1, n_layers = 1, n_hiddens = 20\n",
      "\n",
      ">epoch=20000, l_rate=0.010, error=0.134\n",
      "PREDICTIONS:\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "EXPECTED:\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "Accuracy = 59.091%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4 7]\n",
      " [2 9]]\n",
      "TN=4, FP=7, FN=2, TP=9\n",
      "Elapsed Time: 285.3998234272003s\n",
      "\n",
      "==========================================================\n",
      "\n",
      "pca = 10, l_rate = 0.01, loss_limit = 0.1, n_layers = 1, n_hiddens = 30\n",
      "\n",
      ">epoch=20000, l_rate=0.010, error=0.157\n",
      "PREDICTIONS:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1]\n",
      "EXPECTED:\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "Accuracy = 72.727%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [4 7]]\n",
      "TN=9, FP=2, FN=4, TP=7\n",
      "Elapsed Time: 404.4844424724579s\n",
      "\n",
      "==========================================================\n",
      "\n",
      "pca = 10, l_rate = 0.01, loss_limit = 0.1, n_layers = 1, n_hiddens = 40\n",
      "\n",
      ">epoch=20000, l_rate=0.010, error=0.130\n",
      "PREDICTIONS:\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "EXPECTED:\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "Accuracy = 68.182%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 5]\n",
      " [2 9]]\n",
      "TN=6, FP=5, FN=2, TP=9\n",
      "Elapsed Time: 537.3270251750946s\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "target_pca = 10\n",
    "l_rate = 0.01\n",
    "loss_limit = 0.1\n",
    "n_layers = 1\n",
    "\n",
    "n_hiddens = 20\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 30\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 40\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "# n_hiddens = 50\n",
    "# run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "# print('\\n==========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pca = 20, l_rate = 0.01, loss_limit = 0.1, n_layers = 1, n_hiddens = 20\n",
      "\n",
      ">epoch=12156, l_rate=0.010, error=0.100\n",
      "PREDICTIONS:\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1]\n",
      "EXPECTED:\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "Accuracy = 59.091%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 3]\n",
      " [6 5]]\n",
      "TN=8, FP=3, FN=6, TP=5\n",
      "Elapsed Time: 269.1842634677887s\n",
      "\n",
      "==========================================================\n",
      "\n",
      "pca = 20, l_rate = 0.01, loss_limit = 0.1, n_layers = 1, n_hiddens = 30\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f9c92d43d336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mn_hiddens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_pca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n=========================================================='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b8221acfc9b6>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# network = back_propagation_tts(pca_train_set, pca_test_set, l_rate, n_epoch, n_layers, n_hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./train_test_fix/network{}-{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_pca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_train_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpca_test_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hiddens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-73ed6f2e0d4e>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(train, test, l_rate, loss_limit, n_layers, n_hidden, name)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-d33b5c09d9aa>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(network, train, l_rate, loss_limit, n_outputs)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0msum_error\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mbackward_propagate_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mepoch\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m20000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-d33b5c09d9aa>\u001b[0m in \u001b[0;36mupdate_weights\u001b[1;34m(network, row, l_rate)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delta'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_pca = 20\n",
    "l_rate = 0.01\n",
    "loss_limit = 0.1\n",
    "n_layers = 1\n",
    "\n",
    "n_hiddens = 20\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 30\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 40\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "# n_hiddens = 50\n",
    "# run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "# print('\\n==========================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pca = 30\n",
    "l_rate = 0.01\n",
    "loss_limit = 0.1\n",
    "n_layers = 1\n",
    "\n",
    "n_hiddens = 20\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 30\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 40\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "# n_hiddens = 50\n",
    "# run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "# print('\\n==========================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pca = 40\n",
    "l_rate = 0.01\n",
    "loss_limit = 0.1\n",
    "n_layers = 1\n",
    "\n",
    "n_hiddens = 20\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 30\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "n_hiddens = 40\n",
    "run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "print('\\n==========================================================')\n",
    "\n",
    "# n_hiddens = 50\n",
    "# run(target_pca, l_rate, loss_limit, n_layers, n_hiddens)\n",
    "# print('\\n==========================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
